{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0a3b17ec5c917aac5cd265fd6214cd9600f46c379dc3c3ef662a0a5f1a65c1ca0",
   "display_name": "Python 3.8.3 64-bit ('image_caption_generator': virtualenvwrapper)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, image_pretrained_model=None):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.image_pretrained_model = image_pretrained_model\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        mod_path = f'tensorflow.keras.applications.{self.image_pretrained_model[0]}'\n",
    "        pre_mod = importlib.import_module(mod_path)\n",
    "        self.image_model = getattr(pre_mod, \n",
    "            self.image_pretrained_model[1])(include_top=False, weights='imagenet')\n",
    "        self.new_input = self.image_model.input\n",
    "        self.hidden_layer = self.image_model.layers[-1].output\n",
    "        self.image_features_extract_model = tf.keras.Model(self.new_input, \n",
    "            self.hidden_layer)\n",
    "        self.image_features_extract_model.trainable = False\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.image_features_extract_model(x)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectorizer:\n",
    "    def fit(self, texts=None, top_k=5000):\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n",
    "        oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        self.tokenizer.word_index['<pad>'] = 0\n",
    "        self.tokenizer.index_word[0] = '<pad>'\n",
    "    def predict(self, texts=None):\n",
    "        train_seqs = self.tokenizer.texts_to_sequences(texts)\n",
    "        cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, \n",
    "        padding='post')\n",
    "        return cap_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset:\n",
    "    def __init__(self, \n",
    "                 name='COCO', \n",
    "                 image_pretrained_model=('vgg16', 'VGG16')):\n",
    "        self.name = name\n",
    "        self.image_pretrained_model = image_pretrained_model\n",
    "        self.root = '/Users/mamu867/PNNL_Mac/Springboard/image_caption_generator/'\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_file(filename):\n",
    "        _, extension = os.path.splitext(filename)\n",
    "\n",
    "        if extension not in ['.json', '.txt', '.csv']:\n",
    "            print(f'File read for extension {extension} is not yet available.')\n",
    "            return\n",
    "        if extension == '.txt':\n",
    "            with open(filename, 'r') as f:\n",
    "                return f.read()\n",
    "        if extension == '.json':\n",
    "            with open(filename, 'r') as f:\n",
    "                return pd.DataFrame.from_dict(json.load(f)['annotations'])\n",
    "        if extension == '.csv':\n",
    "            return pd.read_csv(filename, delimiter='|')\n",
    "    \n",
    "    def load_annotation(self):\n",
    "        if self.name == 'Flickr30k':\n",
    "            fpath = self.root + 'data/raw/Flickr30k/flickr30k_images/'\n",
    "            self.annotations = self.__class__.read_file(fpath + 'results.csv')\n",
    "            self.annotations['image_path'] = self.annotations.apply(\n",
    "                lambda x: fpath + x['image_name'], axis=1)\n",
    "            self.annotations.rename(columns={' comment': 'caption'}, inplace=True)\n",
    "            self.annotations['image_path'] = self.annotations.apply(\n",
    "                lambda x: fpath + 'flickr30k_images/' + x['image_name'], axis=1)\n",
    "            self.annotations['image_id'] = self.annotations.apply(\n",
    "                lambda x: float(x['image_name'].split('.')[0]),axis=1)\n",
    "            self.annotations.sort_values(by='image_id', inplace=True)\n",
    "            self.annotations.dropna(inplace=True)\n",
    "            self.annotations['caption'] = self.annotations.apply(\n",
    "                lambda x: '<start> ' + x['caption'] + ' <end>', axis = 1)\n",
    "        else:\n",
    "            root_path = self.root + 'data/raw/COCO/'\n",
    "            train_path = root_path + 'annotations/captions_train2014.json'\n",
    "            #val_path = root_path + 'annotations/captions_val2014.json'\n",
    "            self.annotations = self.__class__.read_file(train_path)\n",
    "            self.annotations['image_path'] = self.annotations.apply(\n",
    "                lambda x: root_path + 'train2014/' + 'COCO_train2014_{0:012d}.jpg'.format(x['image_id']), axis=1)\n",
    "            self.annotations.sort_values(by='image_id', inplace=True)\n",
    "            self.annotations.dropna(inplace=True)\n",
    "            self.annotations['caption'] = self.annotations.apply(\n",
    "                lambda x: '<start> ' + x['caption'] + ' <end>', axis = 1)\n",
    "            #self.annotations_val = self.__class__.read_file(val_path)\n",
    "            #self.annotations_val['image_path'] = self.annotations_val.apply(\n",
    "            # lambda x: root_path + 'val2014/' + 'COCO_val2014_{0:012d}.jpg'.format(\n",
    "            # x['image_id']), axis=1)\n",
    "            #self.annotations_val.sort_values(by='image_id', inplace=True)\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def load_image(self, image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        mod_path = f'tensorflow.keras.applications.{self.image_pretrained_model[0]}'\n",
    "        pre_mod = importlib.import_module(mod_path)\n",
    "        img = pre_mod.preprocess_input(img)\n",
    "        return img\n",
    "\n",
    "    def get_image_features(self, f_locs, image_features_extract_model):\n",
    "        X = tf.convert_to_tensor(np.array([self.load_image(f).numpy() \n",
    "        for f in f_locs]), dtype=tf.float64)\n",
    "        batch_features = image_features_extract_model(X)\n",
    "        batch_features = tf.reshape(batch_features, \n",
    "                                (batch_features.shape[0], -1, \n",
    "                                 batch_features.shape[3]))\n",
    "        return batch_features\n",
    "    \n",
    "    def vec_initializer(self, train_size=400, batch_size=64, top_k=10000):\n",
    "        self.wvec = WordVectorizer()\n",
    "        texts = self.annotations['caption'].tolist()[:train_size*batch_size]\n",
    "        self.wvec.fit(texts=texts, top_k=top_k)\n",
    "        self.tokenizer = self.wvec.tokenizer\n",
    "        self.cnn_encoder = CNN_Encoder(self.image_pretrained_model)\n",
    "\n",
    "    def data_processor(self, batch_size=64, n_take=(1000, 200, 200)):\n",
    "        for n, i in enumerate(range(0, len(self.annotations), batch_size), 1):\n",
    "            print('Currently aggregating data for: {}'.format(n))\n",
    "            if self.name == 'COCO':\n",
    "                if n < n_take[0]+1:\n",
    "                    path = self.root + '/data/raw/COCO/train_vectors'\n",
    "                    data_type = 'train'\n",
    "                    count = n \n",
    "                elif n > n_take[0] and n < sum(n_take[:2])+1:\n",
    "                    path = self.root + '/data/raw/COCO/val_vectors'\n",
    "                    data_type = 'val'\n",
    "                    count = n - n_take[0]\n",
    "                elif n > sum(n_take[:2]) and n < sum(n_take)+1:\n",
    "                    path = self.root + '/data/raw/COCO/test_vectors'\n",
    "                    data_type = 'test'\n",
    "                    count = n - n_take[0] - n_take[1]\n",
    "                else:\n",
    "                    break\n",
    "            if self.name == 'Flickr30k':\n",
    "                if n < n_take[0]+1:\n",
    "                    path = self.root + '/data/raw/Flickr30k/flickr30k_images/train_vectors'\n",
    "                    data_type = 'train'  \n",
    "                    count = n \n",
    "                elif n > n_take[0] and n < sum(n_take[:2])+1:\n",
    "                    path = self.root + '/data/raw/Flickr30k/flickr30k_images/val_vectors'\n",
    "                    data_type = 'val'\n",
    "                    count = n - n_take[0]\n",
    "                elif n > sum(n_take[:2]) and n < sum(n_take)+1:\n",
    "                    path = self.root + '/data/raw/Flickr30k/flickr30k_images/test_vectors'\n",
    "                    data_type = 'test'\n",
    "                    count = n - n_take[0] - n_take[1]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            temp_df = self.annotations[i:i+batch_size]\n",
    "            y = self.wvec.predict(temp_df['caption'].tolist())\n",
    "            X = self.get_image_features(temp_df['image_path'].values, self.cnn_encoder)\n",
    "            np.save(os.path.join(path, '{}_{}_{}_X_y_{:04d}_{:012d}_{:012d}.npy'.format(\n",
    "                        data_type,\n",
    "                        self.image_pretrained_model[0], \n",
    "                        self.image_pretrained_model[1], \n",
    "                        count, \n",
    "                        int(temp_df['image_id'].min()), \n",
    "                        int(temp_df['image_id'].max()))), {'X': X, 'y': y})\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Currently aggregating data for: 1\n",
      "Currently aggregating data for: 2\n",
      "Currently aggregating data for: 3\n",
      "Currently aggregating data for: 4\n",
      "Currently aggregating data for: 5\n",
      "Currently aggregating data for: 6\n",
      "Currently aggregating data for: 7\n",
      "Currently aggregating data for: 8\n",
      "Currently aggregating data for: 9\n",
      "Currently aggregating data for: 10\n",
      "Currently aggregating data for: 11\n",
      "Currently aggregating data for: 12\n",
      "Currently aggregating data for: 13\n",
      "Currently aggregating data for: 14\n",
      "Currently aggregating data for: 15\n",
      "Currently aggregating data for: 16\n",
      "Currently aggregating data for: 17\n",
      "Currently aggregating data for: 18\n",
      "Currently aggregating data for: 19\n",
      "Currently aggregating data for: 20\n",
      "Currently aggregating data for: 21\n",
      "Currently aggregating data for: 22\n",
      "Currently aggregating data for: 23\n",
      "Currently aggregating data for: 24\n",
      "Currently aggregating data for: 25\n",
      "Currently aggregating data for: 26\n",
      "Currently aggregating data for: 27\n",
      "Currently aggregating data for: 28\n",
      "Currently aggregating data for: 29\n",
      "Currently aggregating data for: 30\n",
      "Currently aggregating data for: 31\n",
      "Currently aggregating data for: 32\n",
      "Currently aggregating data for: 33\n",
      "Currently aggregating data for: 34\n",
      "Currently aggregating data for: 35\n",
      "Currently aggregating data for: 36\n",
      "Currently aggregating data for: 37\n",
      "Currently aggregating data for: 38\n",
      "Currently aggregating data for: 39\n",
      "Currently aggregating data for: 40\n",
      "Currently aggregating data for: 41\n",
      "Currently aggregating data for: 42\n",
      "Currently aggregating data for: 43\n",
      "Currently aggregating data for: 44\n",
      "Currently aggregating data for: 45\n",
      "Currently aggregating data for: 46\n",
      "Currently aggregating data for: 47\n",
      "Currently aggregating data for: 48\n",
      "Currently aggregating data for: 49\n",
      "Currently aggregating data for: 50\n",
      "Currently aggregating data for: 51\n",
      "Currently aggregating data for: 52\n",
      "Currently aggregating data for: 53\n",
      "Currently aggregating data for: 54\n",
      "Currently aggregating data for: 55\n",
      "Currently aggregating data for: 56\n",
      "Currently aggregating data for: 57\n",
      "Currently aggregating data for: 58\n",
      "Currently aggregating data for: 59\n",
      "Currently aggregating data for: 60\n",
      "Currently aggregating data for: 61\n",
      "Currently aggregating data for: 62\n",
      "Currently aggregating data for: 63\n",
      "Currently aggregating data for: 64\n",
      "Currently aggregating data for: 65\n",
      "Currently aggregating data for: 66\n",
      "Currently aggregating data for: 67\n",
      "Currently aggregating data for: 68\n",
      "Currently aggregating data for: 69\n",
      "Currently aggregating data for: 70\n",
      "Currently aggregating data for: 71\n",
      "Currently aggregating data for: 72\n",
      "Currently aggregating data for: 73\n",
      "Currently aggregating data for: 74\n",
      "Currently aggregating data for: 75\n",
      "Currently aggregating data for: 76\n",
      "Currently aggregating data for: 77\n",
      "Currently aggregating data for: 78\n",
      "Currently aggregating data for: 79\n",
      "Currently aggregating data for: 80\n",
      "Currently aggregating data for: 81\n",
      "Currently aggregating data for: 82\n",
      "Currently aggregating data for: 83\n",
      "Currently aggregating data for: 84\n",
      "Currently aggregating data for: 85\n",
      "Currently aggregating data for: 86\n",
      "Currently aggregating data for: 87\n",
      "Currently aggregating data for: 88\n",
      "Currently aggregating data for: 89\n",
      "Currently aggregating data for: 90\n",
      "Currently aggregating data for: 91\n",
      "Currently aggregating data for: 92\n",
      "Currently aggregating data for: 93\n",
      "Currently aggregating data for: 94\n",
      "Currently aggregating data for: 95\n",
      "Currently aggregating data for: 96\n",
      "Currently aggregating data for: 97\n",
      "Currently aggregating data for: 98\n",
      "Currently aggregating data for: 99\n",
      "Currently aggregating data for: 100\n",
      "Currently aggregating data for: 101\n",
      "Currently aggregating data for: 102\n",
      "Currently aggregating data for: 103\n",
      "Currently aggregating data for: 104\n",
      "Currently aggregating data for: 105\n",
      "Currently aggregating data for: 106\n",
      "Currently aggregating data for: 107\n",
      "Currently aggregating data for: 108\n",
      "Currently aggregating data for: 109\n",
      "Currently aggregating data for: 110\n",
      "Currently aggregating data for: 111\n",
      "Currently aggregating data for: 112\n",
      "Currently aggregating data for: 113\n",
      "Currently aggregating data for: 114\n",
      "Currently aggregating data for: 115\n",
      "Currently aggregating data for: 116\n",
      "Currently aggregating data for: 117\n",
      "Currently aggregating data for: 118\n",
      "Currently aggregating data for: 119\n",
      "Currently aggregating data for: 120\n",
      "Currently aggregating data for: 121\n",
      "Currently aggregating data for: 122\n",
      "Currently aggregating data for: 123\n",
      "Currently aggregating data for: 124\n",
      "Currently aggregating data for: 125\n",
      "Currently aggregating data for: 126\n",
      "Currently aggregating data for: 127\n",
      "Currently aggregating data for: 128\n",
      "Currently aggregating data for: 129\n",
      "Currently aggregating data for: 130\n",
      "Currently aggregating data for: 131\n",
      "Currently aggregating data for: 132\n",
      "Currently aggregating data for: 133\n",
      "Currently aggregating data for: 134\n",
      "Currently aggregating data for: 135\n",
      "Currently aggregating data for: 136\n",
      "Currently aggregating data for: 137\n",
      "Currently aggregating data for: 138\n",
      "Currently aggregating data for: 139\n",
      "Currently aggregating data for: 140\n",
      "Currently aggregating data for: 141\n",
      "Currently aggregating data for: 142\n",
      "Currently aggregating data for: 143\n",
      "Currently aggregating data for: 144\n",
      "Currently aggregating data for: 145\n",
      "Currently aggregating data for: 146\n",
      "Currently aggregating data for: 147\n",
      "Currently aggregating data for: 148\n",
      "Currently aggregating data for: 149\n",
      "Currently aggregating data for: 150\n",
      "Currently aggregating data for: 151\n",
      "Currently aggregating data for: 152\n",
      "Currently aggregating data for: 153\n",
      "Currently aggregating data for: 154\n",
      "Currently aggregating data for: 155\n",
      "Currently aggregating data for: 156\n",
      "Currently aggregating data for: 157\n",
      "Currently aggregating data for: 158\n",
      "Currently aggregating data for: 159\n",
      "Currently aggregating data for: 160\n",
      "Currently aggregating data for: 161\n",
      "Currently aggregating data for: 162\n",
      "Currently aggregating data for: 163\n",
      "Currently aggregating data for: 164\n",
      "Currently aggregating data for: 165\n",
      "Currently aggregating data for: 166\n",
      "Currently aggregating data for: 167\n",
      "Currently aggregating data for: 168\n",
      "Currently aggregating data for: 169\n",
      "Currently aggregating data for: 170\n",
      "Currently aggregating data for: 171\n",
      "Currently aggregating data for: 172\n",
      "Currently aggregating data for: 173\n",
      "Currently aggregating data for: 174\n",
      "Currently aggregating data for: 175\n",
      "Currently aggregating data for: 176\n",
      "Currently aggregating data for: 177\n",
      "Currently aggregating data for: 178\n",
      "Currently aggregating data for: 179\n",
      "Currently aggregating data for: 180\n",
      "Currently aggregating data for: 181\n",
      "Currently aggregating data for: 182\n",
      "Currently aggregating data for: 183\n",
      "Currently aggregating data for: 184\n",
      "Currently aggregating data for: 185\n",
      "Currently aggregating data for: 186\n",
      "Currently aggregating data for: 187\n",
      "Currently aggregating data for: 188\n",
      "Currently aggregating data for: 189\n",
      "Currently aggregating data for: 190\n",
      "Currently aggregating data for: 191\n",
      "Currently aggregating data for: 192\n",
      "Currently aggregating data for: 193\n",
      "Currently aggregating data for: 194\n",
      "Currently aggregating data for: 195\n",
      "Currently aggregating data for: 196\n",
      "Currently aggregating data for: 197\n",
      "Currently aggregating data for: 198\n",
      "Currently aggregating data for: 199\n",
      "Currently aggregating data for: 200\n",
      "Currently aggregating data for: 201\n",
      "Currently aggregating data for: 202\n",
      "Currently aggregating data for: 203\n",
      "Currently aggregating data for: 204\n",
      "Currently aggregating data for: 205\n",
      "Currently aggregating data for: 206\n",
      "Currently aggregating data for: 207\n",
      "Currently aggregating data for: 208\n",
      "Currently aggregating data for: 209\n",
      "Currently aggregating data for: 210\n",
      "Currently aggregating data for: 211\n",
      "Currently aggregating data for: 212\n",
      "Currently aggregating data for: 213\n",
      "Currently aggregating data for: 214\n",
      "Currently aggregating data for: 215\n",
      "Currently aggregating data for: 216\n",
      "Currently aggregating data for: 217\n",
      "Currently aggregating data for: 218\n",
      "Currently aggregating data for: 219\n",
      "Currently aggregating data for: 220\n",
      "Currently aggregating data for: 221\n",
      "Currently aggregating data for: 222\n",
      "Currently aggregating data for: 223\n",
      "Currently aggregating data for: 224\n",
      "Currently aggregating data for: 225\n",
      "Currently aggregating data for: 226\n",
      "Currently aggregating data for: 227\n",
      "Currently aggregating data for: 228\n",
      "Currently aggregating data for: 229\n",
      "Currently aggregating data for: 230\n",
      "Currently aggregating data for: 231\n",
      "Currently aggregating data for: 232\n",
      "Currently aggregating data for: 233\n",
      "Currently aggregating data for: 234\n",
      "Currently aggregating data for: 235\n",
      "Currently aggregating data for: 236\n",
      "Currently aggregating data for: 237\n",
      "Currently aggregating data for: 238\n",
      "Currently aggregating data for: 239\n",
      "Currently aggregating data for: 240\n",
      "Currently aggregating data for: 241\n",
      "Currently aggregating data for: 242\n",
      "Currently aggregating data for: 243\n",
      "Currently aggregating data for: 244\n",
      "Currently aggregating data for: 245\n",
      "Currently aggregating data for: 246\n",
      "Currently aggregating data for: 247\n",
      "Currently aggregating data for: 248\n",
      "Currently aggregating data for: 249\n",
      "Currently aggregating data for: 250\n",
      "Currently aggregating data for: 251\n",
      "Currently aggregating data for: 252\n",
      "Currently aggregating data for: 253\n",
      "Currently aggregating data for: 254\n",
      "Currently aggregating data for: 255\n",
      "Currently aggregating data for: 256\n",
      "Currently aggregating data for: 257\n",
      "Currently aggregating data for: 258\n",
      "Currently aggregating data for: 259\n",
      "Currently aggregating data for: 260\n",
      "Currently aggregating data for: 261\n",
      "Currently aggregating data for: 262\n",
      "Currently aggregating data for: 263\n",
      "Currently aggregating data for: 264\n",
      "Currently aggregating data for: 265\n",
      "Currently aggregating data for: 266\n",
      "Currently aggregating data for: 267\n",
      "Currently aggregating data for: 268\n",
      "Currently aggregating data for: 269\n",
      "Currently aggregating data for: 270\n",
      "Currently aggregating data for: 271\n",
      "Currently aggregating data for: 272\n",
      "Currently aggregating data for: 273\n",
      "Currently aggregating data for: 274\n",
      "Currently aggregating data for: 275\n",
      "Currently aggregating data for: 276\n",
      "Currently aggregating data for: 277\n",
      "Currently aggregating data for: 278\n",
      "Currently aggregating data for: 279\n",
      "Currently aggregating data for: 280\n",
      "Currently aggregating data for: 281\n",
      "Currently aggregating data for: 282\n",
      "Currently aggregating data for: 283\n",
      "Currently aggregating data for: 284\n",
      "Currently aggregating data for: 285\n",
      "Currently aggregating data for: 286\n",
      "Currently aggregating data for: 287\n",
      "Currently aggregating data for: 288\n",
      "Currently aggregating data for: 289\n",
      "Currently aggregating data for: 290\n",
      "Currently aggregating data for: 291\n",
      "Currently aggregating data for: 292\n",
      "Currently aggregating data for: 293\n",
      "Currently aggregating data for: 294\n",
      "Currently aggregating data for: 295\n",
      "Currently aggregating data for: 296\n",
      "Currently aggregating data for: 297\n",
      "Currently aggregating data for: 298\n",
      "Currently aggregating data for: 299\n",
      "Currently aggregating data for: 300\n",
      "Currently aggregating data for: 301\n",
      "Currently aggregating data for: 302\n",
      "Currently aggregating data for: 303\n",
      "Currently aggregating data for: 304\n",
      "Currently aggregating data for: 305\n",
      "Currently aggregating data for: 306\n",
      "Currently aggregating data for: 307\n",
      "Currently aggregating data for: 308\n",
      "Currently aggregating data for: 309\n",
      "Currently aggregating data for: 310\n",
      "Currently aggregating data for: 311\n",
      "Currently aggregating data for: 312\n",
      "Currently aggregating data for: 313\n",
      "Currently aggregating data for: 314\n",
      "Currently aggregating data for: 315\n",
      "Currently aggregating data for: 316\n",
      "Currently aggregating data for: 317\n",
      "Currently aggregating data for: 318\n",
      "Currently aggregating data for: 319\n",
      "Currently aggregating data for: 320\n",
      "Currently aggregating data for: 321\n",
      "Currently aggregating data for: 322\n",
      "Currently aggregating data for: 323\n",
      "Currently aggregating data for: 324\n",
      "Currently aggregating data for: 325\n",
      "Currently aggregating data for: 326\n",
      "Currently aggregating data for: 327\n",
      "Currently aggregating data for: 328\n",
      "Currently aggregating data for: 329\n",
      "Currently aggregating data for: 330\n",
      "Currently aggregating data for: 331\n",
      "Currently aggregating data for: 332\n",
      "Currently aggregating data for: 333\n",
      "Currently aggregating data for: 334\n",
      "Currently aggregating data for: 335\n",
      "Currently aggregating data for: 336\n",
      "Currently aggregating data for: 337\n",
      "Currently aggregating data for: 338\n",
      "Currently aggregating data for: 339\n",
      "Currently aggregating data for: 340\n",
      "Currently aggregating data for: 341\n",
      "Currently aggregating data for: 342\n",
      "Currently aggregating data for: 343\n",
      "Currently aggregating data for: 344\n",
      "Currently aggregating data for: 345\n",
      "Currently aggregating data for: 346\n",
      "Currently aggregating data for: 347\n",
      "Currently aggregating data for: 348\n",
      "Currently aggregating data for: 349\n",
      "Currently aggregating data for: 350\n",
      "Currently aggregating data for: 351\n",
      "Currently aggregating data for: 352\n",
      "Currently aggregating data for: 353\n",
      "Currently aggregating data for: 354\n",
      "Currently aggregating data for: 355\n",
      "Currently aggregating data for: 356\n",
      "Currently aggregating data for: 357\n",
      "Currently aggregating data for: 358\n",
      "Currently aggregating data for: 359\n",
      "Currently aggregating data for: 360\n",
      "Currently aggregating data for: 361\n",
      "Currently aggregating data for: 362\n",
      "Currently aggregating data for: 363\n",
      "Currently aggregating data for: 364\n",
      "Currently aggregating data for: 365\n",
      "Currently aggregating data for: 366\n",
      "Currently aggregating data for: 367\n",
      "Currently aggregating data for: 368\n",
      "Currently aggregating data for: 369\n",
      "Currently aggregating data for: 370\n",
      "Currently aggregating data for: 371\n",
      "Currently aggregating data for: 372\n",
      "Currently aggregating data for: 373\n",
      "Currently aggregating data for: 374\n",
      "Currently aggregating data for: 375\n",
      "Currently aggregating data for: 376\n",
      "Currently aggregating data for: 377\n",
      "Currently aggregating data for: 378\n",
      "Currently aggregating data for: 379\n",
      "Currently aggregating data for: 380\n",
      "Currently aggregating data for: 381\n",
      "Currently aggregating data for: 382\n",
      "Currently aggregating data for: 383\n",
      "Currently aggregating data for: 384\n",
      "Currently aggregating data for: 385\n",
      "Currently aggregating data for: 386\n",
      "Currently aggregating data for: 387\n",
      "Currently aggregating data for: 388\n",
      "Currently aggregating data for: 389\n",
      "Currently aggregating data for: 390\n",
      "Currently aggregating data for: 391\n",
      "Currently aggregating data for: 392\n",
      "Currently aggregating data for: 393\n",
      "Currently aggregating data for: 394\n",
      "Currently aggregating data for: 395\n",
      "Currently aggregating data for: 396\n",
      "Currently aggregating data for: 397\n",
      "Currently aggregating data for: 398\n",
      "Currently aggregating data for: 399\n",
      "Currently aggregating data for: 400\n",
      "Currently aggregating data for: 401\n",
      "Currently aggregating data for: 402\n",
      "Currently aggregating data for: 403\n",
      "Currently aggregating data for: 404\n",
      "Currently aggregating data for: 405\n",
      "Currently aggregating data for: 406\n",
      "Currently aggregating data for: 407\n",
      "Currently aggregating data for: 408\n",
      "Currently aggregating data for: 409\n",
      "Currently aggregating data for: 410\n",
      "Currently aggregating data for: 411\n",
      "Currently aggregating data for: 412\n",
      "Currently aggregating data for: 413\n",
      "Currently aggregating data for: 414\n",
      "Currently aggregating data for: 415\n",
      "Currently aggregating data for: 416\n",
      "Currently aggregating data for: 417\n",
      "Currently aggregating data for: 418\n",
      "Currently aggregating data for: 419\n",
      "Currently aggregating data for: 420\n",
      "Currently aggregating data for: 421\n",
      "Currently aggregating data for: 422\n",
      "Currently aggregating data for: 423\n",
      "Currently aggregating data for: 424\n",
      "Currently aggregating data for: 425\n",
      "Currently aggregating data for: 426\n",
      "Currently aggregating data for: 427\n",
      "Currently aggregating data for: 428\n",
      "Currently aggregating data for: 429\n",
      "Currently aggregating data for: 430\n",
      "Currently aggregating data for: 431\n",
      "Currently aggregating data for: 432\n",
      "Currently aggregating data for: 433\n",
      "Currently aggregating data for: 434\n",
      "Currently aggregating data for: 435\n",
      "Currently aggregating data for: 436\n",
      "Currently aggregating data for: 437\n",
      "Currently aggregating data for: 438\n",
      "Currently aggregating data for: 439\n",
      "Currently aggregating data for: 440\n",
      "Currently aggregating data for: 441\n",
      "Currently aggregating data for: 442\n",
      "Currently aggregating data for: 443\n",
      "Currently aggregating data for: 444\n",
      "Currently aggregating data for: 445\n",
      "Currently aggregating data for: 446\n",
      "Currently aggregating data for: 447\n",
      "Currently aggregating data for: 448\n",
      "Currently aggregating data for: 449\n",
      "Currently aggregating data for: 450\n",
      "Currently aggregating data for: 451\n",
      "Currently aggregating data for: 452\n",
      "Currently aggregating data for: 453\n",
      "Currently aggregating data for: 454\n",
      "Currently aggregating data for: 455\n",
      "Currently aggregating data for: 456\n",
      "Currently aggregating data for: 457\n",
      "Currently aggregating data for: 458\n",
      "Currently aggregating data for: 459\n",
      "Currently aggregating data for: 460\n",
      "Currently aggregating data for: 461\n",
      "Currently aggregating data for: 462\n",
      "Currently aggregating data for: 463\n",
      "Currently aggregating data for: 464\n",
      "Currently aggregating data for: 465\n",
      "Currently aggregating data for: 466\n",
      "Currently aggregating data for: 467\n",
      "Currently aggregating data for: 468\n",
      "Currently aggregating data for: 469\n",
      "Currently aggregating data for: 470\n",
      "Currently aggregating data for: 471\n",
      "Currently aggregating data for: 472\n",
      "Currently aggregating data for: 473\n",
      "Currently aggregating data for: 474\n",
      "Currently aggregating data for: 475\n",
      "Currently aggregating data for: 476\n",
      "Currently aggregating data for: 477\n",
      "Currently aggregating data for: 478\n",
      "Currently aggregating data for: 479\n",
      "Currently aggregating data for: 480\n",
      "Currently aggregating data for: 481\n",
      "Currently aggregating data for: 482\n",
      "Currently aggregating data for: 483\n",
      "Currently aggregating data for: 484\n",
      "Currently aggregating data for: 485\n",
      "Currently aggregating data for: 486\n",
      "Currently aggregating data for: 487\n",
      "Currently aggregating data for: 488\n",
      "Currently aggregating data for: 489\n",
      "Currently aggregating data for: 490\n",
      "Currently aggregating data for: 491\n",
      "Currently aggregating data for: 492\n",
      "Currently aggregating data for: 493\n",
      "Currently aggregating data for: 494\n",
      "Currently aggregating data for: 495\n",
      "Currently aggregating data for: 496\n",
      "Currently aggregating data for: 497\n",
      "Currently aggregating data for: 498\n",
      "Currently aggregating data for: 499\n",
      "Currently aggregating data for: 500\n",
      "Currently aggregating data for: 501\n"
     ]
    }
   ],
   "source": [
    "imdset_coco = ImageDataset('COCO')\n",
    "imdset_coco.load_annotation()\n",
    "imdset_coco.vec_initializer(train_size=400)\n",
    "with open('/Users/mamu867/PNNL_Mac/Springboard/image_caption_generator/data/interim/COCO/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(imdset_coco.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "imdset_coco.data_processor(n_take=(400, 90, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdset_flick = ImageDataset('Flickr30k')\n",
    "# imdset_flick.load_annotation()\n",
    "# imdset_coco.vec_initializer(train_size=160)\n",
    "\n",
    "# with open('/Users/mamu867/PNNL_Mac/Springboard/image_caption_generator/data/interim/fickr30k/tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(imdset_coco.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# imdset_flick.data_processor(n_take=(160, 40, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdset_coco = ImageDataset(name='COCO', image_pretrained_model=('inception_v3', 'InceptionV3'))\n",
    "# imdset_coco.load_annotation()\n",
    "# imdset_coco.vec_initializer(train_size=400)\n",
    "# with open('/Users/mamu867/PNNL_Mac/Springboard/image_caption_generator/data/interim/COCO/tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(imdset_coco.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# imdset_coco.data_processor(n_take=(400, 90, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i in tqdm(range(0, len(self.annotations_val), batch_size)):\n",
    "                if i > n_take[1]:\n",
    "                    break\n",
    "                temp_df = self.annotations_val[i:i+batch_size]\n",
    "                y = wvec.predict(temp_df['caption'].values)\n",
    "                X = self.get_image_features(temp_df['image_path'].values, cnn_encoder)\n",
    "                path = '/content/drive/MyDrive/image_captioning_data/COCO/val2014_image_features'\n",
    "                np.save(os.path.join(path, 'val_{}_{}_X_y_{}_{}.npy'.format(self.image_pretrained_model[0],\n",
    "                                                                              self.image_pretrained_model[1],\n",
    "                                                                              temp_df['image_id'].min(),\n",
    "                                                                              temp_df['image_id'].max())), \n",
    "                        {'X': X, 'y': y})\n",
    "            test_data = sorted(glob('/content/drive/MyDrive/image_captioning_data/COCO/test2014/*'))\n",
    "            for i in tqdm(range(0, len(test_data), batch_size)):\n",
    "                if i > n_take[2]:\n",
    "                    break\n",
    "                temp = test_data[i:i+batch_size]\n",
    "                X = self.get_image_features(temp, cnn_encoder)\n",
    "                path = '/content/drive/MyDrive/image_captioning_data/COCO/test2014_image_features'\n",
    "                np.save(os.path.join(path, 'test_{}_{}_X_{}_{}.npy'.format(self.image_pretrained_model[0],\n",
    "                                                                              self.image_pretrained_model[1],\n",
    "                                                                              temp_df['image_id'].min(),\n",
    "                                                                              temp_df['image_id'].max())), \n",
    "                        {'X': X})\n",
    "'''"
   ]
  }
 ]
}