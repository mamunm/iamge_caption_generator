{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_Models_COCO_inceptionv3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1n0RP2EE8Oq1SZ2ejpy0ODVj6X3iSNP-m",
      "authorship_tag": "ABX9TyMjD2YOJQkSMSMbGMOGlspJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamunm/iamge_caption_generator/blob/main/notebooks/Attention_Models_COCO_inceptionv3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9fxjCpMWIxN",
        "outputId": "4a79e99d-c023-4bee-d8a2-9cdb9af86688"
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219237 sha256=b5e783e630d50154975ee26694c0942aba1cd1ff3601622dc76ac101be95a3bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReI9hlLgBi9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4b1295-8049-47ad-ffa8-24e60fa0f3f0"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuOBotfwGPYp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from glob import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "import pickle5 as pickle\n",
        "import time\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_7gwns-HBxg"
      },
      "source": [
        "# Model settings\n",
        "NAME = 'COCO' # Flicker30k\n",
        "IMAGE_MODEL = 'inception_v3'\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "top_k = 10000\n",
        "vocab_size = top_k + 1\n",
        "num_steps = 400 if NAME == 'COCO' else 160 \n",
        "features_shape = 512 if IMAGE_MODEL == 'vgg16' else 2048 #2048 for inception_v3\n",
        "attention_features_shape = 81 if IMAGE_MODEL == 'vgg16' else 64 #64 for inception_v3\n",
        "max_length = 45 if NAME == 'COCO' else 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VA2Cin4HEh2"
      },
      "source": [
        "# Load the numpy files\n",
        "def load_data(batch, data='COCO', im_model='vgg16'):\n",
        "    if data == 'COCO':\n",
        "        root = '/content/drive/MyDrive/image_captioning_data/COCO/train_vectors/'\n",
        "    else:\n",
        "        root = '/content/drive/MyDrive/image_captioning_data/Flicker30k/train_vectors/'\n",
        "    query = root + '*_{}_*_X_y_{:04d}_*'.format(im_model, batch)\n",
        "    f_path = glob(query)\n",
        "    data = np.load(f_path[0], allow_pickle=True)[()]\n",
        "    return data['X'].numpy(), data['y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3ojhUK3HfhW"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "      super(BahdanauAttention, self).__init__()\n",
        "      self.W1 = tf.keras.layers.Dense(units)\n",
        "      self.W2 = tf.keras.layers.Dense(units)\n",
        "      self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "      hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "      attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "      score = self.V(attention_hidden_layer)\n",
        "      attention_weights = tf.nn.softmax(score, axis=1)\n",
        "      context_vector = attention_weights * features\n",
        "      context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "      return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzOdsJPUHtKV"
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxBD7O9HHwNJ"
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc2(x)\n",
        "        return x, state, attention_weights\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxsLqxjNHzOg"
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzaKrTX0H1_r"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TudYcb17H4wm"
      },
      "source": [
        "checkpoint_path = f\"/content/drive/MyDrive/image_captioning_data/{NAME}/Checkpoints/{NAME + '_' + IMAGE_MODEL}\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdkXtIpHH_XI"
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMpwr-0gIENX"
      },
      "source": [
        "with open(f'/content/drive/MyDrive/image_captioning_data/{NAME}/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SE9l64FIJnx"
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "        for i in range(1, target.shape[1]):\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    return loss, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snbDzpXbIKsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d89f26-893a-46c1-dfe4-5da1b77c5982"
      },
      "source": [
        "loss_plot = []\n",
        "EPOCHS = 20\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    for batch in range(1, num_steps+1):\n",
        "        img_tensor, target = load_data(batch, data=NAME, im_model=IMAGE_MODEL)\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function train_step at 0x7f3fa0025cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function train_step at 0x7f3fa0025cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function train_step at 0x7f3fa0025cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function train_step at 0x7f3fa0025cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function train_step at 0x7f3fa0025cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7f3fa0025cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7f3fa0025cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1 Batch 100 Loss 2.2939\n",
            "Epoch 1 Batch 200 Loss 2.4714\n",
            "Epoch 1 Batch 300 Loss 1.7933\n",
            "Epoch 1 Batch 400 Loss 2.5407\n",
            "Epoch 1 Loss 2.609439\n",
            "Time taken for 1 epoch 978.29 sec\n",
            "\n",
            "Epoch 2 Batch 100 Loss 1.6646\n",
            "Epoch 2 Batch 200 Loss 1.9262\n",
            "Epoch 2 Batch 300 Loss 1.4563\n",
            "Epoch 2 Batch 400 Loss 2.1894\n",
            "Epoch 2 Loss 1.984292\n",
            "Time taken for 1 epoch 201.30 sec\n",
            "\n",
            "Epoch 3 Batch 100 Loss 1.4961\n",
            "Epoch 3 Batch 200 Loss 1.7337\n",
            "Epoch 3 Batch 300 Loss 1.3491\n",
            "Epoch 3 Batch 400 Loss 1.9839\n",
            "Epoch 3 Loss 1.783993\n",
            "Time taken for 1 epoch 200.99 sec\n",
            "\n",
            "Epoch 4 Batch 100 Loss 1.3999\n",
            "Epoch 4 Batch 200 Loss 1.6279\n",
            "Epoch 4 Batch 300 Loss 1.2389\n",
            "Epoch 4 Batch 400 Loss 1.8517\n",
            "Epoch 4 Loss 1.655764\n",
            "Time taken for 1 epoch 201.00 sec\n",
            "\n",
            "Epoch 5 Batch 100 Loss 1.3244\n",
            "Epoch 5 Batch 200 Loss 1.5537\n",
            "Epoch 5 Batch 300 Loss 1.1586\n",
            "Epoch 5 Batch 400 Loss 1.7238\n",
            "Epoch 5 Loss 1.561945\n",
            "Time taken for 1 epoch 201.10 sec\n",
            "\n",
            "Epoch 6 Batch 100 Loss 1.2814\n",
            "Epoch 6 Batch 200 Loss 1.4639\n",
            "Epoch 6 Batch 300 Loss 1.1351\n",
            "Epoch 6 Batch 400 Loss 1.6483\n",
            "Epoch 6 Loss 1.483781\n",
            "Time taken for 1 epoch 201.98 sec\n",
            "\n",
            "Epoch 7 Batch 100 Loss 1.1865\n",
            "Epoch 7 Batch 200 Loss 1.3903\n",
            "Epoch 7 Batch 300 Loss 1.0525\n",
            "Epoch 7 Batch 400 Loss 1.5720\n",
            "Epoch 7 Loss 1.414649\n",
            "Time taken for 1 epoch 201.52 sec\n",
            "\n",
            "Epoch 8 Batch 100 Loss 1.1353\n",
            "Epoch 8 Batch 200 Loss 1.3223\n",
            "Epoch 8 Batch 300 Loss 1.0114\n",
            "Epoch 8 Batch 400 Loss 1.5004\n",
            "Epoch 8 Loss 1.353098\n",
            "Time taken for 1 epoch 201.40 sec\n",
            "\n",
            "Epoch 9 Batch 100 Loss 1.1003\n",
            "Epoch 9 Batch 200 Loss 1.2875\n",
            "Epoch 9 Batch 300 Loss 0.9472\n",
            "Epoch 9 Batch 400 Loss 1.4482\n",
            "Epoch 9 Loss 1.298868\n",
            "Time taken for 1 epoch 200.81 sec\n",
            "\n",
            "Epoch 10 Batch 100 Loss 1.0634\n",
            "Epoch 10 Batch 200 Loss 1.2424\n",
            "Epoch 10 Batch 300 Loss 0.9237\n",
            "Epoch 10 Batch 400 Loss 1.4113\n",
            "Epoch 10 Loss 1.253752\n",
            "Time taken for 1 epoch 201.20 sec\n",
            "\n",
            "Epoch 11 Batch 100 Loss 1.0579\n",
            "Epoch 11 Batch 200 Loss 1.2203\n",
            "Epoch 11 Batch 300 Loss 0.8867\n",
            "Epoch 11 Batch 400 Loss 1.3464\n",
            "Epoch 11 Loss 1.213196\n",
            "Time taken for 1 epoch 201.77 sec\n",
            "\n",
            "Epoch 12 Batch 100 Loss 1.0273\n",
            "Epoch 12 Batch 200 Loss 1.1318\n",
            "Epoch 12 Batch 300 Loss 0.8577\n",
            "Epoch 12 Batch 400 Loss 1.3431\n",
            "Epoch 12 Loss 1.167792\n",
            "Time taken for 1 epoch 201.12 sec\n",
            "\n",
            "Epoch 13 Batch 100 Loss 0.9506\n",
            "Epoch 13 Batch 200 Loss 1.1051\n",
            "Epoch 13 Batch 300 Loss 0.8453\n",
            "Epoch 13 Batch 400 Loss 1.2526\n",
            "Epoch 13 Loss 1.127010\n",
            "Time taken for 1 epoch 201.31 sec\n",
            "\n",
            "Epoch 14 Batch 100 Loss 0.9255\n",
            "Epoch 14 Batch 200 Loss 1.0867\n",
            "Epoch 14 Batch 300 Loss 0.7974\n",
            "Epoch 14 Batch 400 Loss 1.2155\n",
            "Epoch 14 Loss 1.088202\n",
            "Time taken for 1 epoch 201.19 sec\n",
            "\n",
            "Epoch 15 Batch 100 Loss 0.8948\n",
            "Epoch 15 Batch 200 Loss 1.0238\n",
            "Epoch 15 Batch 300 Loss 0.7835\n",
            "Epoch 15 Batch 400 Loss 1.1538\n",
            "Epoch 15 Loss 1.052562\n",
            "Time taken for 1 epoch 200.80 sec\n",
            "\n",
            "Epoch 16 Batch 100 Loss 0.8711\n",
            "Epoch 16 Batch 200 Loss 1.0374\n",
            "Epoch 16 Batch 300 Loss 0.7839\n",
            "Epoch 16 Batch 400 Loss 1.1147\n",
            "Epoch 16 Loss 1.021289\n",
            "Time taken for 1 epoch 201.61 sec\n",
            "\n",
            "Epoch 17 Batch 100 Loss 0.8468\n",
            "Epoch 17 Batch 200 Loss 0.9445\n",
            "Epoch 17 Batch 300 Loss 0.7472\n",
            "Epoch 17 Batch 400 Loss 1.0595\n",
            "Epoch 17 Loss 0.987642\n",
            "Time taken for 1 epoch 201.39 sec\n",
            "\n",
            "Epoch 18 Batch 100 Loss 0.8233\n",
            "Epoch 18 Batch 200 Loss 0.9180\n",
            "Epoch 18 Batch 300 Loss 0.7029\n",
            "Epoch 18 Batch 400 Loss 1.0379\n",
            "Epoch 18 Loss 0.958960\n",
            "Time taken for 1 epoch 201.00 sec\n",
            "\n",
            "Epoch 19 Batch 100 Loss 0.8400\n",
            "Epoch 19 Batch 200 Loss 0.8799\n",
            "Epoch 19 Batch 300 Loss 0.7124\n",
            "Epoch 19 Batch 400 Loss 1.0579\n",
            "Epoch 19 Loss 0.931562\n",
            "Time taken for 1 epoch 201.39 sec\n",
            "\n",
            "Epoch 20 Batch 100 Loss 0.7590\n",
            "Epoch 20 Batch 200 Loss 0.8388\n",
            "Epoch 20 Batch 300 Loss 0.6766\n",
            "Epoch 20 Batch 400 Loss 1.0302\n",
            "Epoch 20 Loss 0.908733\n",
            "Time taken for 1 epoch 200.80 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqWTx3YJgCRL"
      },
      "source": [
        "df_loss = pd.DataFrame({'loss': [i.numpy() for i in loss_plot], 'epoch': np.arange(1, 21)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "2apIkiMXWRLT",
        "outputId": "c9f519bc-5457-49fd-f727-a2c1ef4182f8"
      },
      "source": [
        "sns.lineplot(data=df_loss, x='epoch', y='loss', color='#009473', linewidth=4)\n",
        "plt.xlabel('epoch', fontsize=16)\n",
        "plt.ylabel('training loss', fontsize=16)\n",
        "plt.savefig(f'/content/drive/MyDrive/image_captioning_data/{NAME}/{NAME}_{IMAGE_MODEL}.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy9kho9Dg2x-"
      },
      "source": [
        "def evaluate(img_tensor_val):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lQj3AF9h6pu"
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
        "        grid_size = max(np.ceil(len_result/2), 2)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
        "        ax.set_title(result[i])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ4qRnbV60ES"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PH7lav28h92J",
        "outputId": "00a08aa7-651f-49ad-9db6-32fd17e6e48f"
      },
      "source": [
        "# captions on the validation set\n",
        "#fpath = f'/content/drive/MyDrive/image_captioning_data/{NAME}/val_vectors/'\n",
        "#query = fpath + '*_{}_*'.format(IMAGE_MODEL)\n",
        "#fpath = glob(query)\n",
        "\n",
        "#data = np.load(np.random.choice(fpath), allow_pickle=True)[()]\n",
        "#rid = np.random.choice(90)\n",
        "#X, y = data['X'].numpy()[rid], data['y'][rid]\n",
        "reid = re.compile(r'\\d{12}')\n",
        "a = glob(f'/content/drive/MyDrive/image_captioning_data/{NAME}/val_vectors/val_{IMAGE_MODEL}*')\n",
        "for aa in a:\n",
        "    data = np.load(aa, allow_pickle=True)[()]\n",
        "    X, y = data['X'].numpy()[0], data['y'][0]\n",
        "    if NAME == 'COCO':\n",
        "        iid = reid.findall(aa)[0]\n",
        "        image_id = f'/content/drive/MyDrive/image_captioning_data/COCO/val_images/COCO_train2014_{iid}.jpg'\n",
        "    else:\n",
        "        iid = reid.findall(aa)[0].lstrip('0')\n",
        "        image_id = f'/content/drive/MyDrive/image_captioning_data/Flicker30k/val_images/{iid}.jpg'\n",
        "\n",
        "    real_caption = ' '.join([tokenizer.index_word[i]\n",
        "                        for i in y if i not in [0]])\n",
        "    result, attention_plot = evaluate(X)\n",
        "    print(image_id)\n",
        "    print('Real Caption:', real_caption)\n",
        "    print('Prediction Caption:', ' '.join(result))\n",
        "    plot_attention(image_id, result, attention_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk2olO1YkeDP"
      },
      "source": [
        "val_data_files = glob(f'/content/drive/MyDrive/image_captioning_data/{NAME}/val_vectors/val_{IMAGE_MODEL}*')\n",
        "\n",
        "real = []\n",
        "prediction = []\n",
        "for v in val_data_files:\n",
        "    data = np.load(v, allow_pickle=True)[()]\n",
        "    for x, y in zip(data['X'].numpy(), data['y']):\n",
        "        real.append(' '.join([tokenizer.index_word[i]\n",
        "                        for i in y if i not in [0]]))\n",
        "        result, _ = evaluate(x)\n",
        "        prediction.append(' '.join(result))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spqk-DDe8Zq2"
      },
      "source": [
        "def sub_tok(x):\n",
        "    x = re.sub('<start>', '', x)\n",
        "    x = re.sub('<end>', '', x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-vgbWyJ8jW4"
      },
      "source": [
        "real = [sub_tok(i) for i in real]\n",
        "prediction = [sub_tok(i) for i in real]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCJeo7dDoH9M"
      },
      "source": [
        "real = [i.split() for i in real]\n",
        "prediction = [i.split() for i in prediction]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwGDP40L9jS5"
      },
      "source": [
        "np.save(f'/content/drive/MyDrive/image_captioning_data/{NAME}/{NAME}_{IMAGE_MODEL}_pred.npy', \n",
        "        {'real': real, 'pred': prediction})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgZdh-SI-k4G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}