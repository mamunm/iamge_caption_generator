{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_Models_Flicker30k_inceptionv3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "14TsNrQyrKxtVnWZ7g1hEJj3vo0Dc1wuZ",
      "authorship_tag": "ABX9TyOWK76WRWby6cBGSD56inq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mamunm/iamge_caption_generator/blob/main/notebooks/Attention_Models_Flicker30k_inceptionv3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9fxjCpMWIxN",
        "outputId": "3a784448-5f39-4033-f8da-38a3923aff11"
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 11.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 8.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219264 sha256=403677d7f2d97a790463a9a7cfb4c72990f36483321ec455ce72693aaa8f42b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReI9hlLgBi9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec43fcdd-cc07-4a9a-df4a-13953d54068a"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuOBotfwGPYp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from glob import glob\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "import pickle5 as pickle\n",
        "import time\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_7gwns-HBxg"
      },
      "source": [
        "# Model settings\n",
        "NAME = 'Flicker30k' # Flicker30k\n",
        "IMAGE_MODEL = 'inception_v3'\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "top_k = 10000\n",
        "vocab_size = top_k + 1\n",
        "num_steps = 400 if NAME == 'COCO' else 160 \n",
        "features_shape = 512 if IMAGE_MODEL == 'vgg16' else 2048 #2048 for inception_v3\n",
        "attention_features_shape = 81 if IMAGE_MODEL == 'vgg16' else 64 #64 for inception_v3\n",
        "max_length = 45 if NAME == 'COCO' else 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VA2Cin4HEh2"
      },
      "source": [
        "# Load the numpy files\n",
        "def load_data(batch, data='COCO', im_model='vgg16'):\n",
        "    if data == 'COCO':\n",
        "        root = '/content/drive/MyDrive/image_captioning_data/COCO/train_vectors/'\n",
        "    else:\n",
        "        root = '/content/drive/MyDrive/image_captioning_data/Flicker30k/train_vectors/'\n",
        "    query = root + '*_{}_*_X_y_{:04d}_*'.format(im_model, batch)\n",
        "    f_path = glob(query)\n",
        "    data = np.load(f_path[0], allow_pickle=True)[()]\n",
        "    return data['X'].numpy(), data['y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3ojhUK3HfhW"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "      super(BahdanauAttention, self).__init__()\n",
        "      self.W1 = tf.keras.layers.Dense(units)\n",
        "      self.W2 = tf.keras.layers.Dense(units)\n",
        "      self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "      hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "      attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "      score = self.V(attention_hidden_layer)\n",
        "      attention_weights = tf.nn.softmax(score, axis=1)\n",
        "      context_vector = attention_weights * features\n",
        "      context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "      return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzOdsJPUHtKV"
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxBD7O9HHwNJ"
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        x = self.fc1(output)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc2(x)\n",
        "        return x, state, attention_weights\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxsLqxjNHzOg"
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzaKrTX0H1_r"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TudYcb17H4wm"
      },
      "source": [
        "checkpoint_path = f\"/content/drive/MyDrive/image_captioning_data/{NAME}/Checkpoints/{NAME + '_' + IMAGE_MODEL}\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdkXtIpHH_XI"
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMpwr-0gIENX"
      },
      "source": [
        "with open(f'/content/drive/MyDrive/image_captioning_data/{NAME}/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SE9l64FIJnx"
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "        for i in range(1, target.shape[1]):\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "    return loss, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snbDzpXbIKsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4333992-191b-4a40-cc5e-64332e11d61a"
      },
      "source": [
        "loss_plot = []\n",
        "EPOCHS = 20\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    for batch in range(1, num_steps+1):\n",
        "        img_tensor, target = load_data(batch, data=NAME, im_model=IMAGE_MODEL)\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 7 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:7 out of the last 8 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:8 out of the last 9 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:9 out of the last 10 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 18 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 18 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 16 calls to <function train_step at 0x7fd450165cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Epoch 1 Batch 100 Loss 2.7803\n",
            "Epoch 1 Loss 2.426958\n",
            "Time taken for 1 epoch 1024.46 sec\n",
            "\n",
            "Epoch 2 Batch 100 Loss 2.3766\n",
            "Epoch 2 Loss 2.003562\n",
            "Time taken for 1 epoch 88.62 sec\n",
            "\n",
            "Epoch 3 Batch 100 Loss 2.1755\n",
            "Epoch 3 Loss 1.822711\n",
            "Time taken for 1 epoch 87.21 sec\n",
            "\n",
            "Epoch 4 Batch 100 Loss 2.0359\n",
            "Epoch 4 Loss 1.711744\n",
            "Time taken for 1 epoch 87.69 sec\n",
            "\n",
            "Epoch 5 Batch 100 Loss 1.9577\n",
            "Epoch 5 Loss 1.623250\n",
            "Time taken for 1 epoch 87.69 sec\n",
            "\n",
            "Epoch 6 Batch 100 Loss 1.8700\n",
            "Epoch 6 Loss 1.553102\n",
            "Time taken for 1 epoch 88.64 sec\n",
            "\n",
            "Epoch 7 Batch 100 Loss 1.7914\n",
            "Epoch 7 Loss 1.486082\n",
            "Time taken for 1 epoch 89.76 sec\n",
            "\n",
            "Epoch 8 Batch 100 Loss 1.7526\n",
            "Epoch 8 Loss 1.427735\n",
            "Time taken for 1 epoch 87.22 sec\n",
            "\n",
            "Epoch 9 Batch 100 Loss 1.7004\n",
            "Epoch 9 Loss 1.372173\n",
            "Time taken for 1 epoch 87.89 sec\n",
            "\n",
            "Epoch 10 Batch 100 Loss 1.5773\n",
            "Epoch 10 Loss 1.323345\n",
            "Time taken for 1 epoch 87.82 sec\n",
            "\n",
            "Epoch 11 Batch 100 Loss 1.5158\n",
            "Epoch 11 Loss 1.272740\n",
            "Time taken for 1 epoch 88.83 sec\n",
            "\n",
            "Epoch 12 Batch 100 Loss 1.4778\n",
            "Epoch 12 Loss 1.226101\n",
            "Time taken for 1 epoch 88.56 sec\n",
            "\n",
            "Epoch 13 Batch 100 Loss 1.4088\n",
            "Epoch 13 Loss 1.183168\n",
            "Time taken for 1 epoch 87.61 sec\n",
            "\n",
            "Epoch 14 Batch 100 Loss 1.4083\n",
            "Epoch 14 Loss 1.136584\n",
            "Time taken for 1 epoch 87.57 sec\n",
            "\n",
            "Epoch 15 Batch 100 Loss 1.3146\n",
            "Epoch 15 Loss 1.090685\n",
            "Time taken for 1 epoch 87.72 sec\n",
            "\n",
            "Epoch 16 Batch 100 Loss 1.3037\n",
            "Epoch 16 Loss 1.043832\n",
            "Time taken for 1 epoch 88.11 sec\n",
            "\n",
            "Epoch 17 Batch 100 Loss 1.2452\n",
            "Epoch 17 Loss 1.007854\n",
            "Time taken for 1 epoch 89.19 sec\n",
            "\n",
            "Epoch 18 Batch 100 Loss 1.1710\n",
            "Epoch 18 Loss 0.972786\n",
            "Time taken for 1 epoch 87.10 sec\n",
            "\n",
            "Epoch 19 Batch 100 Loss 1.1233\n",
            "Epoch 19 Loss 0.936392\n",
            "Time taken for 1 epoch 87.70 sec\n",
            "\n",
            "Epoch 20 Batch 100 Loss 1.0555\n",
            "Epoch 20 Loss 0.892217\n",
            "Time taken for 1 epoch 87.80 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqWTx3YJgCRL"
      },
      "source": [
        "df_loss = pd.DataFrame({'loss': [i.numpy() for i in loss_plot], 'epoch': np.arange(1, 21)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "2apIkiMXWRLT",
        "outputId": "137e4da3-87ac-45ac-94b7-6d267db95145"
      },
      "source": [
        "sns.lineplot(data=df_loss, x='epoch', y='loss', color='#009473', linewidth=4)\n",
        "plt.xlabel('epoch', fontsize=16)\n",
        "plt.ylabel('training loss', fontsize=16)\n",
        "plt.savefig(f'/content/drive/MyDrive/image_captioning_data/{NAME}/{NAME}_{IMAGE_MODEL}.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy9kho9Dg2x-"
      },
      "source": [
        "def evaluate(img_tensor_val):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lQj3AF9h6pu"
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
        "        grid_size = max(np.ceil(len_result/2), 2)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
        "        ax.set_title(result[i])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PH7lav28h92J",
        "outputId": "4cb01b56-00db-434b-a03b-106aaf6b8d42"
      },
      "source": [
        "# captions on the validation set\n",
        "#fpath = f'/content/drive/MyDrive/image_captioning_data/{NAME}/val_vectors/'\n",
        "#query = fpath + '*_{}_*'.format(IMAGE_MODEL)\n",
        "#fpath = glob(query)\n",
        "\n",
        "#data = np.load(np.random.choice(fpath), allow_pickle=True)[()]\n",
        "#rid = np.random.choice(90)\n",
        "#X, y = data['X'].numpy()[rid], data['y'][rid]\n",
        "reid = re.compile(r'\\d{12}')\n",
        "a = glob(f'/content/drive/MyDrive/image_captioning_data/{NAME}/val_vectors/val_{IMAGE_MODEL}*')\n",
        "for aa in a:\n",
        "    data = np.load(aa, allow_pickle=True)[()]\n",
        "    X, y = data['X'].numpy()[0], data['y'][0]\n",
        "    if NAME == 'COCO':\n",
        "        iid = reid.findall(aa)[0]\n",
        "        image_id = f'/content/drive/MyDrive/image_captioning_data/COCO/val_images/COCO_train2014_{iid}.jpg'\n",
        "    else:\n",
        "        iid = reid.findall(aa)[0].lstrip('0')\n",
        "        image_id = f'/content/drive/MyDrive/image_captioning_data/Flicker30k/val_images/{iid}.jpg'\n",
        "\n",
        "    real_caption = ' '.join([tokenizer.index_word[i]\n",
        "                        for i in y if i not in [0]])\n",
        "    result, attention_plot = evaluate(X)\n",
        "    print(image_id)\n",
        "    print('Real Caption:', real_caption)\n",
        "    print('Prediction Caption:', ' '.join(result))\n",
        "    plot_attention(image_id, result, attention_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk2olO1YkeDP"
      },
      "source": [
        "val_data_files = glob(f'/content/drive/MyDrive/image_captioning_data/{NAME}/val_vectors/val_{IMAGE_MODEL}*')\n",
        "\n",
        "real = []\n",
        "prediction = []\n",
        "for v in val_data_files:\n",
        "    data = np.load(v, allow_pickle=True)[()]\n",
        "    for x, y in zip(data['X'].numpy(), data['y']):\n",
        "        real.append(' '.join([tokenizer.index_word[i]\n",
        "                        for i in y if i not in [0]]))\n",
        "        result, _ = evaluate(x)\n",
        "        prediction.append(' '.join(result))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spqk-DDe8Zq2"
      },
      "source": [
        "def sub_tok(x):\n",
        "    x = re.sub('<start>', '', x)\n",
        "    x = re.sub('<end>', '', x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-vgbWyJ8jW4"
      },
      "source": [
        "real = [sub_tok(i) for i in real]\n",
        "prediction = [sub_tok(i) for i in real]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCJeo7dDoH9M"
      },
      "source": [
        "#real = [i.split() for i in real]\n",
        "#prediction = [i.split() for i in prediction]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwGDP40L9jS5"
      },
      "source": [
        "np.save(f'/content/drive/MyDrive/image_captioning_data/{NAME}/{NAME}_{IMAGE_MODEL}_pred.npy', \n",
        "        {'real': real, 'pred': prediction})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgZdh-SI-k4G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}